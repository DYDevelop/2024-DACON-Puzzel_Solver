{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9705db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-cache-dir gdown\n",
    "!pip install lightning transformers einops albumentations wandb\n",
    "!pip install --upgrade segmentation-models-pytorch timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a913bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.makedirs('./data', exist_ok = True)\n",
    "# !gdown https://drive.google.com/uc?id=1f9els9TSoZKklXFmW_5C_aSICjvc6-8h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3553c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -qq open.zip -d ./data\n",
    "'''\n",
    "압축 해제가 끝나면 경로는 다음과 같음\n",
    "notebooks\n",
    "├─ data  \n",
    "│  ├─ train # train images directory \n",
    "│  ├─ test # test images directory\n",
    "│  ├─ train.csv\n",
    "│  ├─ test.csv\n",
    "│  └─ sample_submission.csv\n",
    "├─ submissions # submission csv save directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import lightning as L\n",
    "\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange\n",
    "from torchvision.io import read_image, write_jpeg\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b55116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify vit-g (add pos bias, attn bias)\n",
    "from timm.models.vision_transformer import Block, Attention, VisionTransformer\n",
    "\n",
    "def attention_forward(self, x, attn_bias=None):\n",
    "    B, N, C = x.shape\n",
    "    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv.unbind(0)\n",
    "    q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "    q = q * self.scale\n",
    "    attn = q @ k.transpose(-2, -1)\n",
    "    if attn_bias is not None:\n",
    "        attn = attn + attn_bias\n",
    "    attn = attn.softmax(dim=-1)\n",
    "    attn = self.attn_drop(attn)\n",
    "    x = attn @ v\n",
    "\n",
    "    x = x.transpose(1, 2).reshape(B, N, C)\n",
    "    x = self.proj(x)\n",
    "    x = self.proj_drop(x)\n",
    "    return x\n",
    "Attention.forward = attention_forward\n",
    "\n",
    "def block_forward(self, x_and_attn_bias):\n",
    "    x, attn_bias = x_and_attn_bias\n",
    "    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_bias)))\n",
    "    x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "    return (x, attn_bias)\n",
    "Block.forward = block_forward\n",
    "\n",
    "def vision_transformer_forward_features(self, x, embed_bias=None, attn_bias=None):\n",
    "    x = self.patch_embed(x)\n",
    "    x = self._pos_embed(x)\n",
    "    if embed_bias is not None:\n",
    "        x = x + embed_bias\n",
    "    x = self.patch_drop(x)\n",
    "    x = self.norm_pre(x)\n",
    "    x, _ = self.blocks((x,attn_bias))\n",
    "    x = self.norm(x)\n",
    "    return x\n",
    "VisionTransformer.forward_features = vision_transformer_forward_features\n",
    "\n",
    "def vision_transformer_forward(self, x, embed_bias=None, attn_bias=None):\n",
    "    x = self.forward_features(x, embed_bias, attn_bias)\n",
    "    return x\n",
    "VisionTransformer.forward = vision_transformer_forward\n",
    "\n",
    "model = timm.create_model('vit_medium_patch16_gap_256', pretrained=True, num_classes=0)\n",
    "# model = timm.create_model('vit_medium_patch16_gap_384', pretrained=True, num_classes=0)\n",
    "\n",
    "model_config = {\n",
    "    'image_size':256,\n",
    "    'patch_size':16,\n",
    "    'hidden_size':512,\n",
    "    'num_attention_heads':8,\n",
    "}\n",
    "\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomPhotometricDistort(),\n",
    "    transforms.Resize(size=(model_config['image_size'],model_config['image_size']), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.125), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(model_config['image_size'],model_config['image_size']), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['seed']=42\n",
    "config['batch_size']=64\n",
    "config['RHF']=0.5\n",
    "config['RVF']=0.5\n",
    "config['ROTATE'] = 0.5\n",
    "config['CUTOUT'] = 0.0\n",
    "config['RHF_TTA']=0.0\n",
    "config['RVF_TTA']=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d889f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_csv('./data/train.csv')\n",
    "train_df, val_df = train_test_split(total_df, test_size=0.2, random_state=config['seed'])\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class JigsawDataset(Dataset):\n",
    "    def __init__(self, df, data_path, mode='train'):\n",
    "        self.df = df\n",
    "        self.data_path = data_path\n",
    "        self.mode = mode\n",
    "        self.RHF = config['RHF']\n",
    "        self.RVF = config['RVF']\n",
    "        self.ROTATE = config['ROTATE']\n",
    "        self.CUTOUT = config['CUTOUT']\n",
    "        self.RHF_test = config['RHF_TTA']\n",
    "        self.RVF_test = config['RVF_TTA']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(os.path.join(self.data_path, row['img_path']))\n",
    "            shuffle_order = row[[str(i) for i in range(1, 17)]].values-1\n",
    "            image_src = self.reset_image(image, shuffle_order)\n",
    "            \n",
    "            # Random Vertical Flip\n",
    "            if torch.rand(1) < self.RVF:\n",
    "                image_src = np.flipud(image_src.transpose(1,2,0)) # (Tensor[image_height, image_width, image_channels])\n",
    "                image_src = image_src.transpose(2, 0, 1) # (Tensor[image_channels, image_height, image_width])\n",
    "                \n",
    "            # Random Horizontal Flip\n",
    "            if torch.rand(1) < self.RHF:\n",
    "                image_src = np.fliplr(image_src.transpose(1,2,0)) # (Tensor[image_height, image_width, image_channels])\n",
    "                image_src = image_src.transpose(2, 0, 1) # (Tensor[image_channels, image_height, image_width])\n",
    "                \n",
    "            # Random Rotate 90, 180, 270\n",
    "            if torch.rand(1) < self.ROTATE:\n",
    "                times = np.random.randint(1, 4) # 1, 2, 3 -> 90, 180, 270\n",
    "                image_src = np.rot90(image_src.transpose(1,2,0), times) # (Tensor[image_height, image_width, image_channels])\n",
    "                image_src = image_src.transpose(2, 0, 1) # (Tensor[image_channels, image_height, image_width])\n",
    "            \n",
    "            # Random CutOut\n",
    "            if torch.rand(1) < self.CUTOUT:\n",
    "                for i in range(3):\n",
    "                    top_h = int(torch.rand(1) * model_config['image_size'])\n",
    "                    top_w = int(torch.rand(1) * model_config['image_size'])\n",
    "                    h = int(torch.rand(1) * 25)\n",
    "                    w = int(torch.rand(1) * 25)\n",
    "                    image_src[:, top_h : top_h + h, top_w : top_w + w] = 0\n",
    "                \n",
    "                \n",
    "            image_reshuffle, reshuffle_order = self.shuffle_image(image_src)\n",
    "            adjacency_matrix = self.get_adjacency_matrix(reshuffle_order)\n",
    "            data = {\n",
    "                'image_src':image_src,\n",
    "                'image_reshuffle':image_reshuffle,\n",
    "                'order':reshuffle_order,\n",
    "                'adjacency_matrix':adjacency_matrix,\n",
    "                'score': self.get_score(range(16), reshuffle_order),\n",
    "            }\n",
    "            return data\n",
    "        elif self.mode == 'val':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(os.path.join(self.data_path, row['img_path'])).numpy()\n",
    "            shuffle_order = row[[str(i) for i in range(1, 17)]].values-1\n",
    "            adjacency_matrix = self.get_adjacency_matrix(shuffle_order.tolist())\n",
    "            data = {\n",
    "                'image':image,\n",
    "                'order':shuffle_order,\n",
    "                'adjacency_matrix':adjacency_matrix,\n",
    "            }\n",
    "            return data\n",
    "        elif self.mode == 'inference':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(os.path.join(self.data_path, row['img_path'])).numpy() # (Tensor[image_channels, image_height, image_width])\n",
    "            \n",
    "            # Random Vertical Flip TTA\n",
    "            if torch.rand(1) < self.RVF_test:\n",
    "                image = np.flipud(image.transpose(1,2,0)) # (Tensor[image_height, image_width, image_channels])\n",
    "                image = image.transpose(2, 0, 1) # (Tensor[image_channels, image_height, image_width])\n",
    "                \n",
    "            # Random Horizontal Flip TTA\n",
    "            if torch.rand(1) < self.RHF_test:\n",
    "                image = np.fliplr(image.transpose(1,2,0)) # (Tensor[image_height, image_width, image_channels])\n",
    "                image = image.transpose(2, 0, 1) # (Tensor[image_channels, image_height, image_width])\n",
    "                \n",
    "            data = {\n",
    "                'image':image \n",
    "            }\n",
    "            return data\n",
    "\n",
    "    def reset_image(self, image, shuffle_order):\n",
    "        c, h, w = image.shape\n",
    "        block_h, block_w = h//4, w//4\n",
    "        image_src = [[0 for _ in range(4)] for _ in range(4)]\n",
    "        for idx, order in enumerate(shuffle_order):\n",
    "            h_idx, w_idx = divmod(order,4)\n",
    "            h_idx_shuffle, w_idx_shuffle = divmod(idx, 4)\n",
    "            image_src[h_idx][w_idx] = image[:, block_h * h_idx_shuffle : block_h * (h_idx_shuffle+1), block_w * w_idx_shuffle : block_w * (w_idx_shuffle+1)]\n",
    "        image_src = np.concatenate([np.concatenate(image_row, -1) for image_row in image_src], -2)\n",
    "        return image_src\n",
    "\n",
    "    def shuffle_image(self, image):\n",
    "        c, h, w = image.shape\n",
    "        block_h, block_w = h//4, w//4\n",
    "        shuffle_order = list(range(0, 16))\n",
    "        random.shuffle(shuffle_order)\n",
    "        image_shuffle = [[0 for _ in range(4)] for _ in range(4)]\n",
    "        for idx, order in enumerate(shuffle_order):\n",
    "            h_idx, w_idx = divmod(order,4)\n",
    "            h_idx_shuffle, w_idx_shuffle = divmod(idx, 4)\n",
    "            image_shuffle[h_idx_shuffle][w_idx_shuffle] = image[:, block_h * h_idx : block_h * (h_idx+1), block_w * w_idx : block_w * (w_idx+1)]\n",
    "        image_shuffle = np.concatenate([np.concatenate(image_row, -1) for image_row in image_shuffle], -2)\n",
    "        return image_shuffle, shuffle_order\n",
    "\n",
    "    def get_adjacency_matrix(self, order): # 패치에 대하여 연결된 패치 찾기\n",
    "        order_matrix = [order[4*i:4*(i+1)]for i in range(4)]\n",
    "        adj_matrix = np.zeros((16,16), dtype=int)\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                o = order_matrix[i][j]\n",
    "                i_o, j_o = divmod(o,4)\n",
    "                for i_add,j_add in [(-1,0), (1,0), (0,1), (0,-1)]:\n",
    "                    i_compare, j_compare = i_o+i_add, j_o+j_add\n",
    "                    if i_compare<0 or i_compare>=4 or j_compare<0 or j_compare>=4 : continue\n",
    "                    o_compare = order[i_compare*4+j_compare]\n",
    "                    i_, j_ = i*4+j, order.index(i_compare*4+j_compare)\n",
    "                    if (i_add,j_add) == (-1,0):\n",
    "                        adj_matrix[i_][j_] = 1 # 상\n",
    "                        adj_matrix[j_][i_] = 2 # 하\n",
    "                    elif (i_add,j_add) == (-1,0):\n",
    "                        adj_matrix[i_][j_] = 2\n",
    "                        adj_matrix[j_][i_] = 1\n",
    "                    elif  (i_add,j_add) == (0,-1):\n",
    "                        adj_matrix[i_][j_] = 3 # 좌\n",
    "                        adj_matrix[j_][i_] = 4 # 우\n",
    "                    elif (i_add,j_add) == (0,1):\n",
    "                        adj_matrix[i_][j_] = 4\n",
    "                        adj_matrix[j_][i_] = 3\n",
    "        return adj_matrix\n",
    "\n",
    "    def get_score(self, order_true, order_pred): # regression task? 현재 아키텍처와 맞지 않을듯\n",
    "        puzzle_a = np.array(order_true, dtype=int).reshape(4, 4)\n",
    "        puzzle_s = np.array(order_pred, dtype=int).reshape(4, 4)\n",
    "\n",
    "        accuracies = {}\n",
    "        accuracies['1x1'] = np.mean(puzzle_a == puzzle_s)\n",
    "\n",
    "        combinations_2x2 = [(i, j) for i in range(3) for j in range(3)]\n",
    "        combinations_3x3 = [(i, j) for i in range(2) for j in range(2)]\n",
    "\n",
    "        for size in range(2, 5):  # Loop through sizes 2, 3, 4\n",
    "            correct_count = 0  # Initialize counter for correct full sub-puzzles\n",
    "            total_subpuzzles = 0\n",
    "            combinations = combinations_2x2 if size == 2 else combinations_3x3 if size == 3 else [(0, 0)]\n",
    "            for start_row, start_col in combinations:\n",
    "                rows = slice(start_row, start_row + size)\n",
    "                cols = slice(start_col, start_col + size)\n",
    "                if np.array_equal(puzzle_a[rows, cols], puzzle_s[rows, cols]):\n",
    "                    correct_count += 1\n",
    "                total_subpuzzles += 1\n",
    "\n",
    "            accuracies[f'{size}x{size}'] = correct_count / total_subpuzzles\n",
    "\n",
    "        score = (accuracies['1x1'] + accuracies['2x2'] + accuracies['3x3'] + accuracies['4x4']) / 4.\n",
    "        return score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawCollateFn:\n",
    "    def __init__(self, transform, mode):\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if self.mode=='train':\n",
    "            pixel_values = torch.stack([self.transform(Image.fromarray(data['image_reshuffle'].astype(np.uint8).transpose(1,2,0))) for data in batch])\n",
    "            order = torch.LongTensor(np.array([data['order'] for data in batch], dtype=np.float16))\n",
    "            adjacency_matrx = torch.LongTensor(np.array([data['adjacency_matrix'] for data in batch]))\n",
    "            return {\n",
    "                'pixel_values':pixel_values,\n",
    "                'order':order,\n",
    "                'adjacency_matrx':adjacency_matrx\n",
    "            }\n",
    "        elif self.mode=='val':\n",
    "            pixel_values = torch.stack([self.transform(Image.fromarray(data['image'].astype(np.uint8).transpose(1,2,0))) for data in batch])\n",
    "            order = torch.LongTensor(np.array([data['order'] for data in batch], dtype=np.float16))\n",
    "            adjacency_matrx = torch.LongTensor(np.array([data['adjacency_matrix'] for data in batch]))\n",
    "            return {\n",
    "                'pixel_values':pixel_values,\n",
    "                'order':order,\n",
    "                'adjacency_matrx':adjacency_matrx\n",
    "            }\n",
    "        elif self.mode=='inference':\n",
    "            pixel_values = torch.stack([self.transform(Image.fromarray(data['image'].astype(np.uint8).transpose(1,2,0))) for data in batch])\n",
    "            return {\n",
    "                'pixel_values':pixel_values,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bafcdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JigsawDataset(\n",
    "    df=total_df,\n",
    "    data_path='./data',\n",
    "    mode='train'\n",
    ")\n",
    "val_dataset = JigsawDataset(\n",
    "    df=val_df,\n",
    "    data_path='./data',\n",
    "    mode='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, collate_fn=JigsawCollateFn(train_transform, 'train'), batch_size=config['batch_size'], num_workers=11)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=JigsawCollateFn(val_transform, 'val'), batch_size=config['batch_size'], num_workers=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawElectra(nn.Module):\n",
    "    \"\"\"\n",
    "    1st Stage:\n",
    "    In the initial stage, a transformer architecture is employed to discern optimal patch arrangements for each puzzle segment.\n",
    "    This involves intricate spatial relationships, where the model dynamically identifies neighboring patches in cardinal directions(i.e., up, down, left, right).\n",
    "    The foundation of this stage lies in the incorporation of attention matrices at the final layer, providing nuanced insights into patch interdependencies.\n",
    "    \n",
    "    2nd Stage:\n",
    "    Subsequently, the second stage capitalizes on the predicted matrices from the initial stage to derive piece-type embeddings and connect-type embedding.\n",
    "    These embeddings encapsulate diverse spatial configurations, such as cross shapes, left corners and right, and else.\n",
    "    The innovation lies in the integration of piece-type embeddings as positional embedding biases, enhancing the model's contextual awareness.\n",
    "    Furthermore, connect matrix embeddings serve as attention biases, enabling the model to capture intricate inter-piece relationships.\n",
    "    The final objective of this stage is to predict an optimal reordering sequence, leveraging the acquired embeddings.\n",
    "    \n",
    "    The backbone model shares weights excluding head layers. And losses are jointly computed for gradient updates, aiming for efficient learning and high performance.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, config):\n",
    "        super(JigsawElectra, self).__init__()\n",
    "        for k,v in config.items():\n",
    "            setattr(self,k,v)\n",
    "        self.attention_head_size = int(self.hidden_size / self.num_attention_heads)\n",
    "        self.num_patch_per_block = int(self.image_size/4/self.patch_size)\n",
    "        self.model = model\n",
    "        \n",
    "        self.pos_emb = nn.Parameter(torch.randn(16, self.hidden_size))\n",
    "        self.piece_type_emb = nn.Embedding(10, self.hidden_size, padding_idx=0)\n",
    "        self.piece_type_emb.weight.data[0,:]=0\n",
    "        self.piece_type_emb.weight.data = self.piece_type_emb.weight.data*0.1\n",
    "        self.connect_type_emb = nn.Embedding(5, self.num_attention_heads, padding_idx=0)\n",
    "        self.connect_type_emb.weight.data[0,:]=0\n",
    "        self.connect_type_emb.weight.data = self.connect_type_emb.weight.data*0.1\n",
    "        \n",
    "        self.local_linear1 = nn.LazyLinear(self.hidden_size)\n",
    "        self.local_linear2 = nn.LazyLinear(self.hidden_size)\n",
    "        self.local_conv = nn.Conv2d(self.num_attention_heads, self.num_attention_heads, int(self.image_size/16), int(self.image_size/16))\n",
    "        self.local_clf = nn.Sequential(\n",
    "            nn.LazyLinear(self.num_attention_heads),\n",
    "            nn.Tanh(),\n",
    "            nn.LazyLinear(5),\n",
    "        )\n",
    "\n",
    "        self.global_conv = nn.Conv1d(self.hidden_size, self.hidden_size, int(self.image_size/16), int(self.image_size/16))\n",
    "        self.global_clf = nn.Sequential(\n",
    "            nn.LazyLinear(self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.LazyLinear(16),\n",
    "        )\n",
    "\n",
    "    def _transpose(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        b, h, l, d = x.shape\n",
    "        x = torch.cat(x.reshape(b, h, -1, self.num_patch_per_block, d).split(self.num_patch_per_block, 2), 3).reshape(b, h, l, d)\n",
    "        return x\n",
    "        \n",
    "    def local_forward(self, x, label=None):\n",
    "        pos_emb = self.pos_emb.reshape(4,4,-1)\n",
    "        pos_emb = pos_emb.unsqueeze(-2).repeat(1,1,self.num_patch_per_block,1).reshape(4,-1,self.hidden_size)\n",
    "        pos_emb = pos_emb.unsqueeze(1).repeat(1,self.num_patch_per_block, 1, 1).reshape(-1, 4*self.num_patch_per_block, self.hidden_size)\n",
    "        pos_emb = pos_emb.reshape(-1, self.hidden_size)\n",
    "        \n",
    "        x = self.model(x, embed_bias=pos_emb)\n",
    "        x1 = self._transpose(self.local_linear1(x))\n",
    "        x2 = self._transpose(self.local_linear2(x))\n",
    "        x = torch.matmul(x1,x2.transpose(-1, -2)).transpose(-1,-2)\n",
    "        x = self.local_conv(x)\n",
    "        x = x.permute(0,2,3,1)\n",
    "        x = self.local_clf(x)\n",
    "        probs = nn.Softmax(dim=-1)(x)\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = nn.CrossEntropyLoss()(x.reshape(-1, 5), label.reshape(-1))\n",
    "        return x, probs, loss\n",
    "        \n",
    "    def global_forward(self, x, piece_type=None, connect_type=None, label=None):\n",
    "        pos_emb = self.pos_emb.reshape(4,4,-1)\n",
    "        pos_emb = pos_emb.unsqueeze(-2).repeat(1,1,self.num_patch_per_block,1).reshape(4,-1,self.hidden_size)\n",
    "        pos_emb = pos_emb.unsqueeze(1).repeat(1,self.num_patch_per_block, 1, 1).reshape(-1, 4*self.num_patch_per_block, self.hidden_size)\n",
    "        pos_emb = pos_emb.reshape(-1, self.hidden_size)\n",
    "        \n",
    "        if piece_type is not None:\n",
    "            b = piece_type.shape[0]\n",
    "            piece_emb = self.piece_type_emb(piece_type).reshape(b, 4, 4, -1)\n",
    "            piece_emb = piece_emb.unsqueeze(-2).repeat(1,1,1,self.num_patch_per_block,1).reshape(b, 4,-1,self.hidden_size)\n",
    "            piece_emb = piece_emb.unsqueeze(2).repeat(1,1,self.num_patch_per_block, 1, 1).reshape(b,-1, 4*self.num_patch_per_block, self.hidden_size)\n",
    "            piece_emb = piece_emb.reshape(b,-1, self.hidden_size)\n",
    "            pos_emb = piece_emb+pos_emb\n",
    "            \n",
    "        attn_bias = None\n",
    "        if connect_type is not None:\n",
    "            b = connect_type.shape[0]\n",
    "            attn_bias = self.connect_type_emb(connect_type) # B 16,16,8\n",
    "            attn_bias = attn_bias.unsqueeze(-2).repeat(1,1,1,int(self.image_size/16),1).reshape(b,16,-1,self.num_attention_heads)\n",
    "            attn_bias = attn_bias.unsqueeze(2).repeat(1,1,int(self.image_size/16), 1, 1).reshape(b,-1, self.image_size, self.num_attention_heads)\n",
    "            attn_bias = attn_bias.permute(0,3,1,2)\n",
    "            \n",
    "        x = self.model(\n",
    "            x,\n",
    "            embed_bias=pos_emb,\n",
    "            attn_bias=attn_bias,\n",
    "        )\n",
    "        x = self._transpose(x)\n",
    "        b, h, l, d = x.shape\n",
    "        x = x.permute(0,1,3,2).reshape(b,h*d,l)\n",
    "        x = self.global_conv(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.global_clf(x)\n",
    "        probs = nn.Softmax(dim=-1)(x)\n",
    "        \n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = nn.CrossEntropyLoss()(x.reshape(-1, 16), label.reshape(-1))\n",
    "        return x, probs, loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73cbba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "class LitJigsawElectra(L.LightningModule):\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.jigsaw_electra = JigsawElectra(model, config)\n",
    "        self.inference_iter = 1\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        scheduler=CosineAnnealingLR(opt, T_max=10,  eta_min=1e-7, last_epoch=-1)\n",
    "        \n",
    "        # Define the lr_scheduler_step method to update the learning rate of the optimizer\n",
    "        def lr_scheduler_step(self, epoch):\n",
    "            scheduler.step()\n",
    "            return {'lr': scheduler.get_last_lr()}\n",
    "        \n",
    "        return {'optimizer': opt, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'step', 'monitor': 'val_loss'}}\n",
    "        \n",
    "    def training_step(self, batch):\n",
    "        x_local, x_local_probs, loss_local = self.jigsaw_electra.local_forward(batch['pixel_values'], batch['adjacency_matrx'])        \n",
    "        connect_type = x_local_probs.argmax(-1).detach()\n",
    "        piece_type = self.connect_to_piece(connect_type).detach()\n",
    "        x_global, x_global_probs, loss_global = self.jigsaw_electra.global_forward(batch['pixel_values'], piece_type=piece_type, connect_type=connect_type, label=batch['order'])\n",
    "        loss = loss_local*0.2 + loss_global\n",
    "        self.log(\"train_loss_local\", loss_local, on_step=True, on_epoch=False)\n",
    "        self.log(\"train_loss_global\", loss_global, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x_local, x_local_probs, loss_local = self.jigsaw_electra.local_forward(batch['pixel_values'], batch['adjacency_matrx'])\n",
    "        self.log(\"val_loss_local\", loss_local)\n",
    "        connect_type = x_local_probs.argmax(-1).detach()\n",
    "        piece_type = self.connect_to_piece(connect_type).detach()\n",
    "        local_accuracy = torch.mean(1*(connect_type == batch['adjacency_matrx']), dtype=torch.float32)\n",
    "        self.log(\"val_acc_local\", local_accuracy)\n",
    "        x_global, x_global_probs, loss_global = self.jigsaw_electra.global_forward(batch['pixel_values'], piece_type=piece_type, connect_type=connect_type, label=batch['order'])\n",
    "        self.log(\"val_loss_global\", loss_global)\n",
    "        self.validation_step_outputs.append((x_global_probs, batch['order']))\n",
    "        return\n",
    "    \n",
    "    def predict_step(self, batch):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        label = batch.get('order', None)\n",
    "        for i in range(self.inference_iter):\n",
    "            x_local, x_local_probs, _ = self.jigsaw_electra.local_forward(pixel_values)        \n",
    "            connect_type = x_local_probs.argmax(-1).detach()\n",
    "            piece_type = self.connect_to_piece(connect_type).detach()\n",
    "            x_global, x_global_probs, _ = self.jigsaw_electra.global_forward(batch['pixel_values'], piece_type=piece_type, connect_type=connect_type)\n",
    "            reorder = self._probs_to_order(x_global_probs)\n",
    "            pixel_values = self._reorder_image(pixel_values, reorder)\n",
    "        return x_global_probs, reorder, label\n",
    "    \n",
    "    def connect_to_piece(self, connect_types):\n",
    "        device = connect_types.device\n",
    "        connect_types = connect_types.detach().cpu()\n",
    "        piece_types = []\n",
    "        for connect_type in connect_types:\n",
    "            piece_type = []\n",
    "            for connect_type_row in connect_type:\n",
    "                connect_bins = torch.bincount(connect_type_row)\n",
    "                if torch.equal(connect_bins[1:5], torch.LongTensor([0,1,0,1])): #  ┌\n",
    "                    piece_type.append(1)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([0,1,1,1])): # ㅜ\n",
    "                    piece_type.append(2)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([0,1,1,0])): # ㄱ\n",
    "                    piece_type.append(3)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([1,1,0,1])): # ㅏ\n",
    "                    piece_type.append(4)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([1,1,1,0])): # ㅓ\n",
    "                    piece_type.append(5)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([1,0,0,1])): # ㄴ\n",
    "                    piece_type.append(6)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([1,0,1,1])): # ㅗ\n",
    "                    piece_type.append(7)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([1,0,1,0])): # ┘\n",
    "                    piece_type.append(8)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([1,1,1,1])): # +\n",
    "                    piece_type.append(9)\n",
    "                else: # unknown\n",
    "                    piece_type.append(0)\n",
    "            piece_types.append(piece_type)\n",
    "        piece_types = torch.LongTensor(piece_types).to(device)\n",
    "        return piece_types\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        order_pred = []\n",
    "        order_true = []\n",
    "        for probs, order in self.validation_step_outputs:\n",
    "            order_pred.append(self._probs_to_order(probs))\n",
    "            order_true.append(order)\n",
    "        order_pred = torch.cat(order_pred).detach().cpu().numpy()\n",
    "        order_true = torch.cat(order_true).detach().cpu().numpy()\n",
    "        \n",
    "        score, accuracies = self._get_score(order_true, order_pred)\n",
    "\n",
    "        self.log(\"val_score_1x1\", accuracies['1x1'])\n",
    "        self.log(\"val_score\", score)\n",
    "        self.validation_step_outputs.clear()\n",
    "        return\n",
    "    \n",
    "    def _get_score(self, order_true, order_pred):\n",
    "        combinations_2x2 = [(i, j) for i in range(3) for j in range(3)]\n",
    "        combinations_3x3 = [(i, j) for i in range(2) for j in range(2)]\n",
    "        accuracies = {}\n",
    "        accuracies['1x1'] = np.mean(order_true == order_pred)\n",
    "        \n",
    "        for size in range(2, 5): \n",
    "            correct_count = 0  \n",
    "            total_subpuzzles = 0\n",
    "            for i in range(len(order_true)):\n",
    "                puzzle_a = order_true[i].reshape(4, 4)\n",
    "                puzzle_s = order_pred[i].reshape(4, 4)\n",
    "                combinations = combinations_2x2 if size == 2 else combinations_3x3 if size == 3 else [(0, 0)]\n",
    "                for start_row, start_col in combinations:\n",
    "                    rows = slice(start_row, start_row + size)\n",
    "                    cols = slice(start_col, start_col + size)\n",
    "                    if np.array_equal(puzzle_a[rows, cols], puzzle_s[rows, cols]):\n",
    "                        correct_count += 1\n",
    "                    total_subpuzzles += 1\n",
    "            accuracies[f'{size}x{size}'] = correct_count / total_subpuzzles\n",
    "        score = (accuracies['1x1'] + accuracies['2x2'] + accuracies['3x3'] + accuracies['4x4']) / 4.\n",
    "        return score, accuracies\n",
    "        \n",
    "    def _probs_to_order(self, probs): # Greedily arrange the jigsaw puzzle pieces based on maximum probability.\n",
    "        order = []\n",
    "        for prob in probs:\n",
    "            prob = prob.reshape(16,16).clone()\n",
    "            indices = [-1 for _ in range(16)]\n",
    "            for _ in range(16):\n",
    "                i, j = divmod(int(prob.argmax()),16)\n",
    "                indices[i]=j\n",
    "                prob[i, :] = float('-inf')\n",
    "                prob[:, j] = float('-inf')\n",
    "            order.append(indices)\n",
    "        order = torch.LongTensor(order)\n",
    "        return order\n",
    "    \n",
    "    def _reorder_image(self, images, reorders):\n",
    "        device = images.device\n",
    "        images_reordered = []\n",
    "        for image, reorder in zip(images, reorders):\n",
    "            image = image.cpu().clone().numpy()\n",
    "            reorder = reorder.cpu().clone().numpy()\n",
    "            c, h, w = image.shape\n",
    "            block_h, block_w = h//4, w//4\n",
    "            image_src = [[0 for _ in range(4)] for _ in range(4)]\n",
    "            for idx, order in enumerate(reorder):\n",
    "                h_idx, w_idx = divmod(order,4)\n",
    "                h_idx_shuffle, w_idx_shuffle = divmod(idx, 4)\n",
    "                image_src[h_idx][w_idx] = image[:, block_h * h_idx_shuffle : block_h * (h_idx_shuffle+1), block_w * w_idx_shuffle : block_w * (w_idx_shuffle+1)]\n",
    "            image_reordered = np.concatenate([np.concatenate(image_row, -1) for image_row in image_src], -2)\n",
    "            image_reordered = torch.from_numpy(image_reordered)\n",
    "            images_reordered.append(image_reordered)\n",
    "        images_reordered = torch.stack(images_reordered).to(device)\n",
    "        return images_reordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_score',\n",
    "    mode='max',\n",
    "    dirpath='./checkpoints/',\n",
    "    filename='jigsawelectra-vitgap-{epoch:02d}-{val_score:.4f}',\n",
    "    save_top_k=3,\n",
    "    save_weights_only=False\n",
    ")\n",
    "earlystopping_callback = EarlyStopping(monitor=\"val_score\", mode=\"max\", patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708eee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_jigsaw_electra = LitJigsawElectra(model, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb67b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=200, precision='bf16-mixed', callbacks=[checkpoint_callback, earlystopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75fc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "# lit_jigsaw_electra = LitJigsawElectra.load_from_checkpoint('./checkpoints/jigsawelectra-vitgap-epoch=87-val_score=0.9107.ckpt',model=model, config=model_config)\n",
    "# trainer.fit(lit_jigsaw_electra, train_dataloader, val_dataloader, ckpt_path='./checkpoints/jigsawelectra-vitgap-epoch=102-val_score=0.9931.ckpt') # tensorboard --logdir=./lightning_logs/version_{} 로 모니터링 권장\n",
    "trainer.fit(lit_jigsaw_electra, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f79ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = JigsawDataset(\n",
    "    df=val_df,\n",
    "    data_path='./data',\n",
    "    mode='val'\n",
    ")\n",
    "\n",
    "pred_dataset = JigsawDataset(\n",
    "    df=test_df,\n",
    "    data_path='./data',\n",
    "    mode='inference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset, collate_fn=JigsawCollateFn(val_transform, 'val'), batch_size=config['batch_size'], num_workers=11)\n",
    "pred_dataloader = DataLoader(pred_dataset, collate_fn=JigsawCollateFn(val_transform, 'inference'), batch_size=config['batch_size'], num_workers=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ab407",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_jigsaw_electra = LitJigsawElectra.load_from_checkpoint('./checkpoints/jigsawelectra-vitgap-epoch=89-val_score=0.9933.ckpt',model=model, config=model_config)\n",
    "lit_jigsaw_electra.inference_iter=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea310f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6650ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "tmp_path = './tmp'\n",
    "if os.path.exists(tmp_path): shutil.rmtree(tmp_path)\n",
    "os.makedirs(tmp_path)\n",
    "preds = trainer.predict(lit_jigsaw_electra, pred_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ee233",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_pred = torch.cat([order_pred for pixel_values, order_pred, _ in preds]).cpu().numpy()\n",
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "submission.iloc[:,1:] = order_pred+1\n",
    "submission.to_csv('./test_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5fd7e",
   "metadata": {},
   "source": [
    "# TTA (Test Time Augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('./RHV_RVF_TTA.csv').to_numpy()\n",
    "\n",
    "# # RVF\n",
    "# for submit in submission:\n",
    "#     labels = submit[1:]\n",
    "#     dic = {'1':13, '2':14, '3':15, '4':16, '5':9, '6':10, '7':11, '8':12, '9':5, '10':6, '11':7, '12':8, '13':1, '14':2, '15':3, '16':4}\n",
    "#     outputs = []\n",
    "#     for i in range(3, -1, -1):\n",
    "#         for j in range(4):\n",
    "#             outputs.append(dic[str(labels[i*4+j])])\n",
    "#     submit[1:] = outputs\n",
    "\n",
    "# # RHF\n",
    "# for submit in submission:\n",
    "#     labels = submit[1:]\n",
    "#     dic = {'1':4, '2':3, '3':2, '4':1, '5':8, '6':7, '7':6, '8':5, '9':12, '10':11, '11':10, '12':9, '13':16, '14':15, '15':14, '16':13}\n",
    "#     outputs = []\n",
    "#     for i in range(4):\n",
    "#         for j in range(3, -1, -1):\n",
    "#             outputs.append(dic[str(labels[i*4+j])])\n",
    "\n",
    "#     submit[1:] = outputs\n",
    "    \n",
    "# sample = pd.read_csv('./data/sample_submission.csv')\n",
    "# sample.iloc[:,1:] = submission[:,1:]\n",
    "# sample.to_csv('./test_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7575fb48",
   "metadata": {},
   "source": [
    "# CSV Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ee710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "pred1 = pd.read_csv('./CSVs/0.977.csv').to_numpy() ## Basemodel(Jigsaw_Electra) ALL 19 Epoch\n",
    "pred2 = pd.read_csv('./CSVs/0.975.csv').to_numpy() ## AdamW + CosineALR Train/Valid 80/20\n",
    "pred3 = pd.read_csv('./CSVs/0.983.csv').to_numpy() ## AdamW + CosineALR ALL 43 Epoch\n",
    "pred4 = pd.read_csv('./CSVs/0.981.csv').to_numpy() ## RHF 0.3, RVF 0.3, AdamW + CosineALR Train/Valid 80/20\n",
    "pred5 = pd.read_csv('./CSVs/0.985.csv').to_numpy() ## RHF 0.3, RVF 0.3, AdamW + CosineALR ALL 59 Epoch\n",
    "pred6 = pd.read_csv('./CSVs/0.988.csv').to_numpy() ## RHF 0.3, RVF 0.3, ROTATE 0.3, AdamW + CosineALR ALL 91 Epoch\n",
    "pred7 = pd.read_csv('./CSVs/0.982.csv').to_numpy() ## RVF_TTA model 0.985\n",
    "pred8 = pd.read_csv('./CSVs/0.987.csv').to_numpy() ## RVF_TTA_ver2 model 0.988\n",
    "pred9 = pd.read_csv('./CSVs/0.988_2.csv').to_numpy() ## RHF_TTA\n",
    "pred10 = pd.read_csv('./CSVs/0.987_2.csv').to_numpy() ## RHF_RVF_TTA\n",
    "pred11 = pd.read_csv('./CSVs/0.985_2.csv').to_numpy() ## RHF 0.3, RVF 0.3, ROTATE 0.3, CUTOUT 0.5, AdamW + CosineALR ALL 61 Epoch\n",
    "pred12 = pd.read_csv('./CSVs/0.986.csv').to_numpy() ## RHF 0.3, RVF 0.3, ROTATE 0.3, ColorJitter, AdamW + CosineALR ALL 65 Epoch\n",
    "pred13 = pd.read_csv('./CSVs/0.983_2.csv').to_numpy() ## RHF 0.3, RVF 0.3, ROTATE 0.3, AdamW + CosineALR (1e-7)\n",
    "pred14 = pd.read_csv('./CSVs/0.988_3.csv').to_numpy() ## RHF 0.5, RVF 0.5, ROTATE 0.5, Erase 0.5, AdamW + CosineALR ALL 90 Epoch\n",
    "\n",
    "preds = []\n",
    "for p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13, p14 in zip(pred1, pred2, pred3, pred4, pred5, pred6, pred7, pred8, pred9, pred10, pred11, pred12, pred13, pred14):\n",
    "    pred = []\n",
    "    p1_labels, p2_labels, p3_labels, p4_labels, p5_labels, p6_labels, p7_labels, p8_labels, p9_labels, p10_labels, p11_labels, p12_labels, p13_labels, p14_labels = \\\n",
    "        p1[1:], p2[1:], p3[1:], p4[1:], p5[1:], p6[1:], p7[1:], p8[1:], p9[1:], p10[1:], p11[1:], p12[1:], p13[1:], p14[1:]\n",
    "    for i in range(len(p1_labels)):\n",
    "        dic = Counter([p1_labels[i], p2_labels[i], p3_labels[i], p4_labels[i], p5_labels[i], p6_labels[i], p7_labels[i], p8_labels[i], p9_labels[i], p10_labels[i], \\\n",
    "            p11_labels[i], p12_labels[i], p13_labels[i], p14_labels[i]]).most_common(n=1)[0][0]\n",
    "        pred.append(dic)\n",
    "    preds.append(pred)\n",
    "\n",
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "submission.iloc[:,1:] = preds\n",
    "submission.to_csv('./ensemble_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
